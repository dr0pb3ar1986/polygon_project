import os
import json
import logging
import sys
import glob
from datetime import datetime

# Attempt to import optional dependencies
try:
    from tqdm import tqdm
    import coloredlogs

    USE_VISUALS = True
except ImportError:
    USE_VISUALS = False
    # Define a fallback for tqdm if not installed
    tqdm = lambda x, **kwargs: x
    print("Note: Install 'tqdm' and 'coloredlogs' for progress bars and colored output.")

# --- Configuration ---
INPUT_BASE_DIR = r"Y:\Polygon\data_output\stocks\filings"
# Define a new directory for the NotebookLM output files
OUTPUT_BASE_DIR = r"Y:\Polygon\data_output\stocks\filings_notebooklm"

# --- Logging Setup ---
if USE_VISUALS:
    coloredlogs.install(
        level='INFO',
        fmt='%(asctime)s - %(levelname)s - %(message)s',
        stream=sys.stdout
    )
else:
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )

# --- Section Ordering Logic ---
# Define the preferred order for sections to ensure logical flow in the output text file.
SECTION_ORDER = [
    # 10-K Specific
    'Item_1_Business', 'Item_1A_Risk_Factors', 'Item_1B_Unresolved_Staff_Comments',
    'Item_2_Properties', 'Item_3_Legal_Proceedings', 'Item_7_MDandA',
    'Item_7A_Market_Risk', 'Item_8_Financial_Statements',

    # 10-Q Specific (Part 1)
    'P1_Item_1_Financial_Statements', 'P1_Item_2_MDandA', 'P1_Item_3_Market_Risk',
    'P1_Item_4_Controls_and_Procedures',

    # 10-Q Specific (Part 2)
    'P2_Item_1_Legal_Proceedings', 'P2_Item_1A_Risk_Factors',

    # 8-K Specific (Ordered numerically)
    'Item_1_01_Material_Agreement', 'Item_2_01_Acquisition_or_Disposition',
    'Item_2_02_Results_of_Operations', 'Item_5_02_Departure_of_Directors_Officers',
    'Item_7_01_Reg_FD_Disclosure', 'Item_8_01_Other_Events',
]
# Create a mapping from section ID to an integer rank for sorting.
SECTION_RANK = {sid: rank for rank, sid in enumerate(SECTION_ORDER)}
DEFAULT_RANK = 9999  # For any sections not explicitly defined above


def get_section_rank(section_id):
    """Helper to get the sort rank for a section."""
    return SECTION_RANK.get(section_id, DEFAULT_RANK)


def process_ticker_directory(ticker_dir_path):
    """Loads, sorts, and formats all filings for a single ticker."""
    ticker = os.path.basename(ticker_dir_path)

    all_records = []
    # Look for the specific JSONL files generated by the fetch script
    jsonl_files = glob.glob(os.path.join(ticker_dir_path, "sec_filings_*.jsonl"))

    if not jsonl_files:
        return

    # 1. Load all records
    for filepath in jsonl_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        # Add the rank for sorting later
                        record['_rank'] = get_section_rank(record.get('section_id'))
                        all_records.append(record)
                    except json.JSONDecodeError:
                        logging.warning(f"Skipping invalid JSON line in {filepath}")
        except IOError as e:
            logging.error(f"Error reading file {filepath}: {e}")

    if not all_records:
        return

    # 2. Sort records
    # Primary: Chronological (timestamp), Secondary: Accession (groups filing), Tertiary: Section Rank
    # This ensures filings are in order, and sections within filings are logical.
    # We use default empty strings for missing keys to ensure sort stability.
    all_records.sort(key=lambda x: (x.get('filing_timestamp_utc') or '',
                                    x.get('accession_number') or '',
                                    x.get('_rank')))

    # 3. Format into human-readable text
    # Using Markdown-style headers (##, ###) which NotebookLM interprets well for structure.
    output_text = f"# {ticker} Consolidated SEC Filings\n\n"
    current_accession = None

    for record in all_records:
        accession = record.get('accession_number')

        # Start a new filing block if the accession number changes
        if accession != current_accession:
            current_accession = accession
            output_text += format_filing_header(record)

        # Add the section block
        output_text += format_section(record)

    # 4. Save the consolidated file
    output_filename = f"{ticker}_consolidated.txt"
    output_filepath = os.path.join(OUTPUT_BASE_DIR, output_filename)

    try:
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(output_text)
        # logging.info(f"Successfully consolidated {len(all_records)} sections for {ticker}.")
    except IOError as e:
        logging.error(f"Error writing output file {output_filepath}: {e}")


def format_filing_header(record):
    """Formats the main header for a new filing."""
    header = "\n" + "=" * 80 + "\n"
    header += f"## Filing: {record.get('form_type', 'N/A')}\n\n"

    # Format the date nicely if the timestamp is available
    filing_date = "N/A"
    timestamp = record.get('filing_timestamp_utc')
    if timestamp:
        try:
            # Robustly parse ISO 8601 timestamp
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            filing_date = dt.strftime('%B %d, %Y')
        except ValueError:
            filing_date = timestamp  # Fallback to raw timestamp if parsing fails
        header += f"- **Date Filed:** {filing_date} (UTC: {timestamp})\n"

    if record.get('period_end_date'):
        header += f"- **Period End:** {record.get('period_end_date')}\n"

    # Only show 'items_reported' if the list is not empty (common for 10-K/10-Q)
    if record.get('items_reported'):
        header += f"- **Items Reported (8-K):** {record.get('items_reported')}\n"

    header += f"- **Accession:** {record.get('accession_number', 'N/A')}\n"
    header += f"- **CIK:** {record.get('cik', 'N/A')}\n"
    header += "\n" + "=" * 80 + "\n\n"
    return header


def format_section(record):
    """Formats a specific section header and its content."""
    section_id = record.get('section_id', 'Unknown Section').replace('_', ' ')
    text_content = record.get('text', '')

    # Standardize the display name (e.g., Item 1A Risk Factors)
    display_name = section_id

    # Handle specific prefixes (10-Q Part 1/Part 2) for better readability
    if display_name.startswith("P1 "):
        display_name = f"Part I, {display_name[3:]}"
    elif display_name.startswith("P2 "):
        display_name = f"Part II, {display_name[3:]}"

    section_text = "-" * 80 + "\n"
    section_text += f"### {display_name}\n"
    section_text += "-" * 80 + "\n\n"
    # Ensure there is a blank line after the text content for clear separation
    section_text += text_content + "\n\n\n"
    return section_text


def main_workflow():
    """Orchestrates the conversion process."""
    logging.info("--- Starting JSONL to NotebookLM Text Conversion ---")

    # Ensure output directory exists
    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)

    # Discover ticker directories
    if not os.path.exists(INPUT_BASE_DIR):
        logging.error(f"Input directory not found at {INPUT_BASE_DIR}. Exiting.")
        return

    try:
        # Get a list of immediate subdirectories in the input base directory
        # Use os.scandir for efficient directory iteration
        ticker_dirs = [entry.path for entry in os.scandir(INPUT_BASE_DIR) if entry.is_dir()]

        # Filter out the output directory itself if it happens to be inside the input directory
        ticker_dirs = [path for path in ticker_dirs if os.path.abspath(path) != os.path.abspath(OUTPUT_BASE_DIR)]

    except OSError as e:
        logging.error(f"Error accessing input directory {INPUT_BASE_DIR}: {e}. Exiting.")
        return

    if not ticker_dirs:
        logging.info("No ticker directories found in the input directory. Exiting.")
        return

    logging.info(f"Found {len(ticker_dirs)} ticker directories to process.")

    # Process each directory
    # Use tqdm for progress visualization if available
    iterator = tqdm(ticker_dirs, desc="Consolidating Tickers")

    for ticker_dir_path in iterator:
        process_ticker_directory(ticker_dir_path)

    logging.info(f"--- Conversion Complete. Files saved to: {OUTPUT_BASE_DIR} ---")


if __name__ == "__main__":
    main_workflow()